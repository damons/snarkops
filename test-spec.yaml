---
version: storage.snarkos.testing.monadic.us/v1

id: private-tx-500
name: private tx, 500 rounds, 4 accounts
description: |
  This ledger was built to test private transactions

# instructions for generating this test
generate:
  # genesis generation
  genesis:
    seed: 0
    committee: 5
    committee-balances: 10_000_000_000_000
    additional-accounts: 10
    additional-balances: 100_000_000_000
  # ledger setup
  ledger:
    blocks: 100
    transactions:
      total: 1000
      per-block: 10

  # ahead of time transaction generation
  transactions:
    - file: bulk.json
      total: 1000
      amount: 5_000
      sources: [committee.0]
      destinations: [accounts.*]

---
version: storage.snarkos.testing.monadic.us/v1

# ADDITIONAL storage information to be run for this test
# this will run a second test with the same topology but different ledger
id: public-tx-500
name: public tx, 500 rounds, 800 accounts
description: |
  This ledger was built to test a large volume of accounts

---
version: nodes.snarkos.testing.monadic.us/v1

name: my example topology
description: |
  this has too few validators to work as a network

# additional nodes that can be used as validators/peers
external:
  validator/1@canarynet:
    bft: 11.12.13.14:5000
    node: 11.12.13.14:4130
    rest: 11.12.13.14:3030

  validator/1@devnet:
    bft: 11.12.13.14:5000
    node: 11.12.13.14:4130
    rest: 11.12.13.14:3030

# validator/*[@local]
# validator/*@*
# */*@*

# all
# */*
# [*/*]
# validator/1
# validator/*
# */1
# [validator/1]
# [validator/1-*]

# node topology
nodes:
  validator/1:
    key: committee.0
    height: 100
    validators: [validator/2]
    peers: []

  validator/2:
    key: committee.0
    height: 150
    validators: [validator/1]
    peers: []

  client/1:
    height: 0 # empty ledger
    peers: [validator/1, validator/2]

  client/2:
    height: null # null ledger - create ledger when the node is started
    peers: [client/1]

  # creates a group of nodes, all with the same configuration
  # each client will be named `client/my-cluster-0`, `client/my-cluster-1`, ...
  client/my-cluster:
    replicas: 5
    height: 0
    validators: [validator/1]

  prover/1:
    key: accounts.0
    height: 0
    peers: [client/1]

---
version: infrastructure.snarkos.testing.monadic.us/v1

### CONCEPT: providers as docker hosts

providers.beta:

### CONCEPT: providers as node targets

# providers are the locations for where nodes can be run
# providers include address mappings for specifying how a node will refer to another node

# providers have two main modes:
# host mode: run nodes as processes on this machine, using host networking
# guest mode: spawn nodes as VMs or containers, using bridge or guest networking

# TODO: things to consider:
# - maybe providers are pools of slots for nodes to run in

providers.alpha:
  # local provider runs the nodes as child processes on the same machine
  # networking between providers has to be defined in the `addresses` section
  - name: localhost
    mode: host
    addresses:
      local: 127.0.0.1 # address other nodes with the same provider will use to connect
      my-datacenter: public # public address will be used when nodes in `my-datacenter` provider connect
      default: null # cannot connect from other providers
    provider: localhost
    nodes:
      validator/1:
        bft: 5000:15030 # --bft=0.0.0.0:5000 is passed, but external port is 15030
        node: 4130
      # automatically determine ports for this node
      validator/2: auto

    # pool of port mappings for this node
    ports:
      - 5002:15032
      - 5003:15033
      - 4131:14132
      - 4132:14133
      - 6000

  - name: docker
    mode: guest
    provider: docker
    nodes:
      # spin up nodes but don't provide external addresses for them
      # may need to reconsider how port forwarding/mapping works per-machine
      client/1: default
      client/2: default

  - name: docker2
    mode: guest
    provider: docker
    nodes:
      # spin up nodes but don't provide external addresses for them
      # may need to reconsider how port forwarding/mapping works per-machine
      client/1: default
      client/2: default

  # ssh provider runs the nodes via ssh on a remote machine
  - name: my-datacenter
    mode: host
    # addresses other nodes will use to connect to this node
    addresses:
      local: 127.0.0.1
      default: 11.12.13.14
    provider: ssh
    spec:
      host: myinstance.foo.bar
      username: ubuntu
      key: ~/.ssh/id_ed25519
    nodes:
      - client/1
      - client/2

---
version: timeline.snarkos.testing.monadic.us/v1

# a timeline is a series of events that occur during the test

timeline:
  # online will bring the node online. all nodes are offline by default
  - online: [validator/1, validator/2]
    duration: 10s

  # client/1 will go online, and wait 1 minute
  - online: [client/1]
    duration: 1m

  # after 1 minute has passed, the entire network will shutdown
  - offline: */*
  - offline: */*

  # .await forces the step to wait until the condition is met rather than applying the change immediately
  - online.await: */*
  - offline.await: */*
    # wait at most 10 seconds for the nodes to go offline or kill the test
    timeout: 10s

  # when these nodes come back online, their configurations will be different
  - peers:
      validator/1: []
      validator/2: [client/1]
  - validators:
      client/*: [validator/1]
    # modify the height of offline nodes
    height:
      validator/*: 0 # reset all validators ledger height
      prover/2: 100 # set prover 2 to block 100
      client/*: -5 # rollback the ledger 5 blocks
      client/1: -1hr # rollback the ledger by an hour (based on block timestamp deltas)

  - config:
    - nodes: [validator/1, validator/2]
      peers: []
      height: 0 # reset ledger completely
    - nodes: [client/*]
      validators: [validator/1, validator/2]
      height: -5 # rollback the ledger 5 blocks
    - nodes: [prover/2]
      height: 100 # set prover 2 to block 100
    - nodes: [client/1]
      height: -1hr # rollback the ledger by an hour (based on block timestamp deltas)
    
    - nodes: client/*
      peers: []
      height: 0 # reset ledger completely
      


  # wait for 5 blocks to be produced (technically impossible with all nodes offline. may want to add a check for this or end the test if nothing happens after a few seconds)
  - duration: 5

  - expect-height.await:
    validator/1: 500 # wait until validator 1 to be at block 500
    client/1: '+500' # wait for the client to sync 500 blocks


  # wait for all nodes to sync 500 blocks, starting at the beginning
  # 
  - config.await:
    - nodes: client/*
      height: 0
  - duration:
      nodes: client/*
      height-delta: 500 # 5 
      height: 600
    timeout: 2500s # {{block_time * 500 / 2 + 60}}s

  # start firing 100 transactions total at a rate of 5 per second
  # this will not halt execution unless `duration:` is specified alongside `cannons:`
  - cannons:
      - nodes: validator/1
        source: ./testing/0001/pool.json
        total: 100
        tps: 5

  # after all nodes have shutdown, all nodes will come back online with initial configured peers
  - online: */*
    duration: 5s

---
version: timeline.snarkos.testing.monadic.us/v1

# additional timelines can be defined for additional tests

timeline:
  - online: */*
    # this timeline will until a node reaches block height 100
    duration: 100

---
version: outcomes.snarkos.testing.monadic.us/v1

# expected outcomes for the test
metrics:
  network/average-tps: 10
  client/sync-speed: 20
  validator/sync-speed: 20


---
NOTES: |-
  need to create a service for running snarkos that does the following:
  - gather CPU/MEM/Storage/latency facts about the runner machine
  - syncs latest ledger, genesis, and keys to the node from the control plane
  - modifies the ledger and genesis files to be the latest files
  - consumes a JSON/yaml config and converts into snarkos cli flags
  - start/stop snarkos in the desired mode
  - return latest state, logs, and metrics of the node
  - generate transactions given the current genesis/ledger and stream the output back

DREAMS: |-
  control plane node is a webrtc signaling server,
  clients in the swarm can interact via tunneling node traffic through the signaling server
  
  agents can download genesis block/ledger/snarkOS binary from other agents, instead of only the control plane

ARCHITECTURE: |-
  components:
    - worker (node):
      - is launched with:
        - available ports
        - available modes (validator/client/prover)
        - region/zone label
        - auth token for control plane

      - gathers information about the worker machine
      - syncs latest ledger, genesis, and keys to the node from the control plane
      - updates node configuration for peers/validators/ports 
      - applies modifications to the ledger for testing sync
      - starts/stops the node
      - upload logs and metrics to the control plane
      - generates transactions/(solutions?) given a ledger/genesis block
      - receive transactions and post them to the current node
      - [maybe] tunnel TCP node traffic through webrtc to avoid port forwarding/firewalls

    - control plane:
      - connects to workers and orchestrates their operating state
      - view status of workers
      - aggregates logs/metrics from workers
      - manages the state of running tests
      - receives test bundles and runs them on pools of workers
      - instances of a test output artifacts (logs&metrics)
      - run a tx cannon on a given ledger/genesis
      - emit transactions from a file to a specific node
      - [maybe] function as a signaling server between peers

      implementation details:
        maintain the state of all workers
        when a worker's state changes, emit the event to the worker
        allow workers to query the control plane for the current state
        
        timeline actions will can update the state for workers
        timeline actions can wait for collective worker state to meet expectations (maybe with timeout?)


    - cli
      - communicates with the control plane
      - tests:
        - creates test bundles from files
        - uploads to control plane
        - monitors test state
      - view live logs/metrics from runners (through the control plane)
      - interfaces 

  MVP:
    worker: 
      copy latest ledger/genesis/config
      start/stop a snarkos node given the config
      truncate/reset ledger
      receive transaction and broadcast to the running node
      generate and upload transactions



    control plane:
      start a single active test
      walk through the timeline
      configure nodes based on the test topology (get correct IPs and ports)
      gather logs


    cli:
      upload a test bundle to control plane
      monitor & download results from control plane
      request transaction generation from pool given a genesis+ledger
